* Sampling Methods
** Basic Terminology
*** Raw Data
Term used for numbers and category labels that have not been processed yet.
*** Variable
A characteristic that can differ from one individual to another.
*** Observational Unit
A single individual who participated in the study. More simply, an individual may be called an observation.
*** Observation
May also be used to describe the value of a single measurement.
*** Sample Size
The total number of observational units.
*** Dataset
The complete set of raw data, for all observational units and variables in a survey or experiment.
*** Sample Data
Data that is taken from a subset of the population.
*** Population Data
    Data taken from the entire population.
*** Confounder
When 2 groups differ with respect to some factor other than the treatment *and* the factor affects the treatment, we say that the effect is confounded with the treatment.
** Types of Variables
*** Categorical
Categorical characteristics like gender. It can be called an ordinal variable when it has ordered categories, like rating one's skills from 1 to 5, with 5 being the most skillful.(nominal variable means non-ordinal, where there is no inherent order).
*** Numerical
**** Discrete
Discrete variables which are either measurements or counts taken on an individual. Examples are years of education, number of classes taken, etc.
**** Continuous
Used for numerical variables where every value within a range is a possible result. Examples are like height.
** Experiments
*** Experimental Studies
The ideal experiment should consist of entirely randomized controls, and is double-blind. Double-blindedness is important because the placebo effect is well-documented. Also, for example a doctor might be less likely to diagnose a patient positive for a disease if he knew that the patient has been vaccinated before.
*** Observational Studies
Sometimes it's not practical to conduct an experimental study. Example is smoking, because it is not ethical to get people to smoke just for the sake of the experiment. The results of an observational study indicate *association, not causation*.
** Sampling
Two types of sampling: probability sampling and non-probability sampling.
*** Probability Sampling
Involve planned use of chance. Normally more difficult to implement, but eliminates subjectivity and is more likely to give us a representative sample.
**** Simple-random
Draw each sample unit at random. Each unit has an equal chance of being sampled. Any subset of k individuals have the same chance of being sampled as any other subset of k individuals. It is the gold-standard, but often hard/costly to implement.
**** Systematic
Ordered list, starting with a number and then sample all individuals of a particular interval. Need to ensure that the ordered list does not conceal any pattern that might affect the outcome.
**** Cluster
Divide population into subgroups that are representative of the population. Randomly select clusters, and then sample every individual in that cluster. Example: dengue prevention inspection.
**** Stratified
Like quota sampling but makes use of chance instead of judgment. Split population into mutually-exclusive groups and then use simple random sampling. 
*** Non-probability Sampling
Often more convenient to obtain, but need to be mindful of the pitfalls.
**** Quota
Split population into mutually-exclusive groups and sample them proportionally. Important to be careful of the variables and conditions of splitting.
**** Convenience
Example: Getting students in a lecture to raise up their hands. Not representative of population.
**** Judgment
Make your own judgment of what is an appropriate sample. For example, stand outside a canteen and sample people who pass by. Judgment might introduce bias, even when done by an expert.

* Making Sense of Data
There are two steps in making sense of data:
** Graphical Display of Distributions
*** Categorical Variables
**** Pie Charts
Area (and angle) of each slice corresponds to the percentage of each outcome.
Pie charts are a very bad way of reperenting information. The eye is much better judging linear measures rather than relative areas. Bar charts or pareto charts are much better.
**** Bar Charts
- Length of each bar indicates frequency.
- Fixed width.
- Horizontal or vertical.
- To draw attention to relative proportions, can replace count with percentage frequency.
**** Pareto Charts
- Vertical bar chart plotted in descending order.
- Cumulative line graph on the same diagram.
- Can use different axes for bar and line graph (label carefully!).
- Using common axis alows for both count and percentage to be displayed in one slide.
- Quickly identifies the major components and their cumulative share.

*** Quantitative Variables
**** Histograms
- Split the range of data into intervals called bins.
- Count the number of observations of each bin.
- Plot either counts or relative frequencies of each bin.
**** Stem-and-leaf Plots
- Contains two plots separated by a vertical line.
- Left column is stem and right is column.
- These are especially useful when you make them by hand for batches of fewer than a few hundred values.
- Quick way to display and even record numbers.

5|6\\
6|0444\\
6|8888\\
7|2222\\
7|6666\\
8|000044\\
8|8\\

Stem-and-leaf plots help to find even more data from numerical aspects rather than the shape. Notice that these are all even numbers. Perhaps it could be because heartbeats are taken for 15 seconds and multiplied by 4, rather than recorded for the whole duration of 60 seconds?
**** Dotplots
- Dots are plotted on a scale.
- A dot for each observation.
- Visual summary of information when data is not too large.
- May not be able to identify patterns if data is too small.
- Useful to find unusual features.

** Numerical Measures (summary/descriptive statistics)
Two types of descriptive/summary statistics:
*** Measures of Location
Statistics that consider where the data are centered.
**** Mean
The average. Its limitation is that it is oversensitive to extreme values. In which case, it may not be a representative of the location of the majority of sample points.
**** Median
The midpoint. If even number of data, then take the average of the middle two. Its advantage is that it is insensitive to extreme values. However, it is determined mainly by middle points and is less sensitive to the actual numeric points of the remaining data points.
**** Mode
Measure of the most frequent data point. Note: Some distributions have more than one mode. In fact, a useful form of classifying distributions is the number of modes present. Unimodal, bimodal, and trimodal, etc.

The key drawback is that it is ineffective when there are a large number of points, each of which occur infrequently. In such cases, mode will occur far off the center or in extreme cases, will not exist.

If all data points appear once or the same number of times, it is said that *there is no mode*.
*** Measures of Scale or Spread
Statistics that describe variability. For example, variance, range, IQR.
**** Range
Difference between the maximum and minimum data point. It is easy to compute, but ignores all information between the two extremes. It is also very sensitive to extreme values. Another disadvantage is that it depends on the sample size. The larger the size, the bigger the range tends to be. This complication makes it difficult to compare the ranges of sets of differing size.
**** Quantiles and Percentiles
The *pth percentile* is a value where p percent of values fall below that value. The 50th percentile is the median. The more specific definition of the pth percentile is as such:
1. The (k+1)th largest sample point if np/100 is not an integer (where k is the largest integer less than np/100, and n is the sample size);
2. The average of the (np/100)th and the ((np/100)+1)th largest observations if np/100 is an integer.

Percentiles are also known quantiles. The three most useful percentiles are the quartiles. First quartile has p = 25; second quartile is p = 50; and third quartile is where p = 75.

**** Sample Variance (Standard Deviation)
One good way to measure spread is to be:

\begin{equation}
d = \frac{1}{n} \sum\limits_{i=1}^n (x_i - \bar{x})
\end{equation}

The problem with this is that the sum of all deviations about the mean will always be zero. To mitigate this, square it.

\begin{equation}
s^2 = \frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \bar{x})^2
\end{equation}

The *sample standard deviation*, s, is the positive square root of the sample variance. The computational formula for s^2 is an efficient way to compute it:

\begin{equation}
s ^2 = \frac{1}{n-1} \left[\sum\limits_{i=1}^n x_i^2 - \frac{(\sum\limits_{i=1}^n x_i)^2}{n} \right]
\end{equation}
\begin{equation}
s^2 = \frac{1}{n-1} \left[\sum\limits_{i=1}^n x_i^2 - n \bar{x}^2 \right]
\end{equation}

Note that the *sample* variance is not really the variance of the sample. It is just a tool to estimate the *population* variance. It is the mean square distance to the population mean. But we don't usually have the population mean so we use the sample mean instead.

**** IQR (Inter-quartile Range)
Quartiles are also used to define a measure of spread that is more resistant to outliers compared to range and standard deviation.
IQR is the difference between Q3 and Q1. It is not affected by any measurements below Q1 and above Q3

**** Coefficient of Variation

This is a relative measure of variation. It is expressed as a percentage and is hence useable across differences in precision. For example, using kilograms in a cooking recipe is meaningless. The equation is:

\begin{equation}
CV = 100\% \times \frac{s}{|\bar{x}|}
\end{equation}
**** Boxplots
Also know as a box-and-whisker diagram, it depicts groups of numerical data through their five-number summaries: minimum, maximum, Q1, Q2 (median), and Q3. Note that the min and max exclude outliers, which are data points that are either lower than 1.5 x IQR below Q1, or higher than 1.5 x IQR above Q3.

Boxplots are usually plotted together with stem-and-leaf or histograms. Boxplots offer a balance of simplicity and information. They are often plotted side-by-side for groups or categories we wish to compare.
* Probability
** Basic Terminology
*** Sample Space
The set of all possible outcomes of an experiment. For example, the sample space of tossing a coin is S = {H,T}. The lifetime of a transistor S = [0, infinity).
*** Event
Any subset of the sample space is an event.
** Algebra of Events
*** Union
For two events E and F of a sample space S, we defined the union of E and F to be a set consisting of all point that are either in E or F, or both.
*** Intersect
We define EF (or E n F) to be the intersection of  E and F, which consists of all outcomes that are in *both* E and F. Should there be no intersection, then it will be equal to null. If E n F = Ø, then E and F are said to be mutually exclusive.
*** More than 2 Events
\begin{equation}
\bigcup\limits_{1}^\infty E_{n}
\end{equation}

and
\begin{equation}
\bigcap\limits_{1}^\infty E_{n}
\end{equation}
*** Complement
Shown as
\begin{equation}
E^c
\end{equation}
E^c will only occur when E does not happen. S^c = Ø.
*** Subset
\begin{equation}
E \subset F
\end{equation}

if E is subset of F, and F is subset of E, then E = F.
*** Common Laws
\begin{equation}
\text{Commutative laws: } E \cup F = F \cup E \\
E \cap F = F \cap E \\
\text{Associative laws: } (E \cup F) \cup G = E \cup (F \cup G) \\
(E \cap F) \cap G = E \cup (F \cap G) \\
\text{Distributive laws: } (E \cup F)G = (E \cap G) \cup (F \cap G) \\
(E \cap F) \cup G = (E \cup G) \cap (F \cup G)
\end{equation}
One easy way to remember: think of intersection as multiplication and union as addition.
** Counting Methods
*** The Generalized Principle of Counting
Assuming for each experiment i, there are n_{i} possible outcomes. Then for all r experiments, the total number of possible outcomes is the multiplication of all n_{i}.
*** Factorial
With n items, there are n! ways of arranging them, because n x (n-1) x (n-2) etc.
*** Permutations
When the order matters, use nPr.
\begin{equation}
P_{r}^n = \frac{n!}{(n-r)!}
\end{equation}
Example: How many ways to have 2 students (order matters) out of 4? use 4P2.
*** Combinations
When order doesn't matter, use choose. nCr.
\begin{equation}
C_{r}^n = \frac{n!}{r!(n-r)!}
\end{equation}
Example: How many pairs (order doesn't matter) can you get out of 4 students? Answer is 4C2.
** Interpretations of Probability
*** Equally-likely outcomes
If there are m equally-likely possibilities and s of them are regarded as successful, then the probability of success is s/m.
*** Frequency Analysis
Use the proportion of events that has occured compared to all events over a long period of time with repeated experiments.
*** Personal Probability
Subjective method of determining probability using a person's assessment of the chances of succes. For example, a man estimates a 95% chance that his girlfriend will accept his marriage proposal.
** Axioms of Probability
1) For any event A, 0 <= P(A) <= 1.
2) Let S be the sample space, then P(S) = 1.
3) For any two *mutually exclusive* events A and B, P(A \cup B) = P(A) + P(B)

With these 3 axioms, some propositions can be derived.
1) P(Ø) = 0
2) For any finite sequence of mutually *exclusive events*, P(A_{1} \cup A_{2} \cup A_{3 ...}) = P(A_{1}) + P(A_{2}) + P(A_{3}) ...
3) P(A^c) = 1 - P(A)
4) if A ⊆ B, then P(A) + P(B \cap A^c) = P(B). Hence, P(A) ≤ P(B).
5) Let A and B be any two events, then P(A \cup B) = P(A) + P(B) - P(A \cap B).

** Computing Probability
*** Birthday Problem
There are n people in a room, what is the probability that there are at least 2 people who have the same birthday?

\vert{}S\vert{} = 365^n. Let A denote the event that there are at least 2 people in the room who share the same birthday, and A^c denote the complement of that.

To find A^c, we have P(A^c) = $\frac{365 * 364* 363...}{365^n}$.

You will notice that at n = 50, there is a 98% chance that you will be able to find two people with the same birthday in the room.
*** Inverse Birthday Problem
How many people does it take to be in a room for you to have at least a 50% chance of finding someone who shares the same birthday as you?

The probability that all n persons have different birthdays as you (A^c) is $\frac{364}{365}^n$.

So we need n such that $1 - (\frac{364}{365})^n \ge 0.5$.

$n \ge \frac{log(0.5)}{log(\frac{364}{365})} = 252.7$
** Conditional Probability
For any two events A and B where $P(A) \gt 0$, the conditional probability of B given that A has occured is defined as:

\begin{equation}
P(B|A) = \frac{P(A \cap B)}{P(A)}
\end{equation}

*** Multiplication Rule
Following that, we can deduce the multiplication rule:

\begin{equation}
P(A \cap B) = P(B|A)P(A)
\end{equation}
*** Inverse Probability
\begin{equation}
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}
\end{equation}

** Independence
Two events A and B are said to be independent if and only if $P(A \cap B) = P(A)P(B)$.

This is is equivalent to the following statements:
\begin{equation}
P(A|B) = P(A) \\
P(B|A) = P(B)
\end{equation}

** Bayes' Theorem
Before going into Bayes' Theorem, we need to understand the following definitions:
*** Partitions
if B_{1},B_{2},B_{3} are mutually exclusive events and B_{1} \cup B_{2} \cup B_{3} ... = S, then we call B_{1},B_{2},B_{3},...B_{n} a partition of S.

*** Rule of Total Probability
if B_{1},B_{2},B_{3},...B_{n} is a partition of S, then for any event A:
\begin{equation}
P(A) = P(B_{1})P(A|B_{1}) + ... + P(B_{n})P(A|B_{n}) \\
= \sum\limits_{i=1}^n P(B_{i})P(A|B_{i})
\end{equation}

*** Bayes' Theorem
\begin{equation}
P(B_{k}|A) = \frac{P(B_{k})P(A|B_{k})}{P(B_{1})P(A|B_{1}) + ... + P(B_{n})P(A|B_{n})}
\end{equation}

* Discrete Random Variables
** Definitions
A random variable X is a mapping from the sample space to real numbers.
| Outcome            | (T,T) | (T,H) | (H,T) | (H,H) |
| Random Variable, X |     0 | 1     | 1     | 2     |

Example: We toss 3 fair coins. Y denotes number of heads appearing. 

$P(Y = 0) = P(T,T,T) = \frac{1}{8}$.
** Notations
We use capital letters for random variables. (X, Y, etc).

We use  lower-cased letters for values of random variables. Eg {X = x} means that the random variable X takes on the value X.

Technically, we are referring to the event C where C = {s \in S: X(s) = x}.

Example: Consider a single coin toss. S = {H, T}. If the coin is fair, then P(H) = P(T) = 0.5

Define a random variable X(H) = 1, X(T) = 0. Then we have {X = 1} = {H}, and {X = 0} = {T}.

And so P(X = 1) = P({H}) = 0.5, P(X = 0) = P({T}) = 0.5

The probability mass function of X is f, where f(0) = f(1) = 0.5

** Probability Mass function
The probability mass function (pmf) of a discrete random variable X is given by:

\begin{equation}
f(x_{i}) = p(x_{i}) = p_{i} = P({X = x_{i}})
\end{equation}

*** Properties of a Probability Mass function
1) $f(x_{i}) \ge 0$ for every x_{i}.
2) $\sum_{x_{i}} f(x_{i}) = 1$.
3) $P(X \in E) = \sum_{x_{i} \in E} f(x_{i})$.

** Expected Values
The expected value is the mean value that we will get. It is defined by:

\begin{equation}
E(X) = \sum\limits_{x_{i}} x_{i} P(X = x_{i})
\end{equation}

We often denote this as µ. That is, µ = E(X).

*** Properties of Expectation
1) if a and b are constants then E(a + bX) = a + bE(X)
2) For any random variables X and Y, E(X + Y) = E(X) + E(Y)
3) For any function f(x), $E(f(X)) = \sum\limits_{x_{i}} f(x_{i})P(X = x_{i})$.

In particular, when f(x) = x^2,
\begin{equation}
E(X^2) = \sum\limits_{x_{i}} x_{i}^2 P(X = x_{i})
\end{equation}

** Variance
The variance of a random variable X is the number:

\begin{equation}
Var(X) = E[(X-E(X))^2] \\
= \sum\limits_{x_{i}}(x_{i} - µ)^2 P(X = x_{i})
\end{equation}

The standard deviation is the positive square root of the variance.

*** Properties of Variance
1) Var(X) ≥ 0
2) If Var(X) = 0, then P(X = E(X)) = 1
3) If a and b are constants, Var(a + bX) = b^2 Var(X)
4) Var(X) = E(X^2) - (E(X))^2

** Bernoulli Random Variables
*** The Bernoulli Trial
A Bernoulli Trial is an experiment where there are only two outcomes, one of which is success and another is failure.

A Bernoulli process consists of repeatedly performing independent and identical Bernoulli Trials.

*** Bernoulli Random Variable
Let p denote the probability of success. The Bernoulli Random Variable has probability mass function given by:

P(X = 1) = p, P(X = 0) = 1 - p.

E(X) = p, and Var(X) = p(1 - p).

** Binomial Random Variable
A binomial random variable counts the number of successes in n trials in a Bernoulli Process. Suppose we have n trials where:

1) the probability of success for each trial is the same p
2) the trials are independent

then we say X has a binomial distribution and write it as X~B(n,p).

The probability of getting x successes is given as:

\begin{equation}
P(X = x) = \binom{n}{x}p^x (1 - p)^{n-x}
\end{equation}

It can be shown that E(X) = np, and Var(X) np(1 - p).

** Geometric Random Variable
The geometric random variable is the number of Bernoulli trials needed to get the first success.

We write X ~ Geom(p) when the trials have a probability of success p. Then,

\begin{equation}
P(X = k) = (1 - p)^{k - 1}p
\end{equation}

It can be shown that $P(X) = \frac{1}{p}$ and $Var(X) = \frac{1 - p}{p^2}$.

** Poisson Random Variables
The Poisson Random Variable is a discrete probability distribution that expresses the probability of the number of events ocurring in a fixed period of time (or fixed region).

X is a Poisson random variable with parameter λ, denoted by X ~ Poisson(λ), where λ > 0 is the expected number of occurences during the period/region, when its probability mass function is given by:

\begin{equation}
P(X = k) = \frac{e^{-λ}λ^k}{k!}
\end{equation}

It can be shown that E(X) = λ and Var(X) = λ.

** Poisson Approximation of Binomial Distribution
if X ~ B(n, p) with n is very large and p is very small, then the distribution may be approximated by the Poisson distribution X ~ Poisson(np).

Morp precisely, X_{n} ~ B(n, λ/n), Y ~ Poisson(λ).

Then for all 0 ≤ k ≤ n:

\begin{equation}
\lim_{n\to\infty} P(X_{n} = k) = P(Y = k)
\end{equation}

The approximation is good when n ≥ 20 and p ≤ 0.05, or if n ≥ 100 and np ≤ 10.

** Poisson Process
The poisson process is a continuous time process. We count the number of occurences within some interval of time. The defining properties of a Poisson Process with rate parameter α are:

- the expected number of occurences in an interval of Length T is αT
- there are no simultaneous occurences
- the number of occurences in disjoint time intervals are independent

The number of occurences in any interval of a Poisson Process has a Poisson distribution.

** Cumulative Distribution Function
The cumulative distribution function (cdf) for a random variable is F(x) = P(x ≤ x). 

Because the cdf function extends naturally to other types of random variables, it is also known simply as the *distribution function*.

* Tutorials
** Tutorial 1
*** 1
**** a
Population data.
**** b
Sample data.
*** 2
**** a
| Statistical Population                 | Sample                | Variable of Interest |
|----------------------------------------+-----------------------+----------------------|
| Engineers graduating from a university | 20 graduating seniors | Starting salary      |


**** b
| Statistical Population  | Sample   | Variable of Interest     |
|-------------------------+----------+--------------------------|
| 6000 manufactured chips | 50 chips | Whether defective or not |

**** c
| Statistical Population                           | Sample                            | Variable of Interest |
|--------------------------------------------------+-----------------------------------+----------------------|
| All specimens that can be made with the material | 20 specimens made of the material | Tensile strength     |

*** 3
**** a
Discrete because set number of hours (assuming no decimals).
**** b
Continuous because any value between range is possible.
**** c
Categorical. Only a few colours.
**** d
Categorical. Yes or No.
**** e
Categorical-ordinal.
*** 4
No. Convenience sampling. May introduce biases like selecting only people who pass by Platypus Food Bar at S16. (By right answer is "self-selected sampling". This is when investigators just leave the form there for everyone to participate if they want. The problem with this is that those who tend to participate tend to have strong opinions.)
*** 5
**** a
Randomized experiment.
**** b
Observational study.
**** c
Randomized experiment.
**** d
Observational study.
*** 6
**** a
Yes. Affects both parties. Number of modules affects amount of sleep and grades.
**** b
Not confounder. Weight doesnt affect grades nor sleep directly.
**** c
Not confounder. Partying affects sleep, but not necessarily grades.
Ans: Spend energy partying might have less energy studying.
*** 7
**** a
Convenience sampling. (Ans: Not convenience because for convenience sampling, convenience is for the respondents not the investigator. Self-select or volunteer sampling.)
**** b
No. Biased towards people who have strong opinion who are willing to take the time to respond to the poll.
